<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://miraiwu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://miraiwu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-15T10:32:57+08:00</updated><id>https://miraiwu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">InftyDedup (FAST 23) 论文阅读</title><link href="https://miraiwu.github.io/paperreading/2023/03/01/InftyDedup-(FAST-23).html" rel="alternate" type="text/html" title="InftyDedup (FAST 23) 论文阅读"/><published>2023-03-01T00:00:00+08:00</published><updated>2023-03-01T00:00:00+08:00</updated><id>https://miraiwu.github.io/paperreading/2023/03/01/InftyDedup%20(FAST%2023)</id><content type="html" xml:base="https://miraiwu.github.io/paperreading/2023/03/01/InftyDedup-(FAST-23).html"><![CDATA[<p><a href="https://www.usenix.org/conference/fast23/presentation/kotlarska">InftyDedup: Scalable and Cost-Effective Cloud Tiering with Deduplication</a></p> <p>这方面了解不多，所以会记录一些知识：）</p> <h2 id="abstract">Abstract</h2> <ul> <li>maximize scalability by utilizing cloud services not only for storage but also for computation</li> <li>save cost by selecting between hot and cold cloud storage based on the characteristics of each data chunk</li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li>Deduplication between different local tier systems is not supported for data stored in the cloud.</li> <li>Deduplication typically entails chunking data collections into smaller pieces that may be referenced multiple times, thereby possibly having different access patterns. This calls for finer-grained and more automated approaches to storage type selection.</li> </ul> <p>Conclusion:</p> <ul> <li>Like the existing tiering-to-cloud backup solutions, InftyDedup moves selected data from a local-tier system to the cloud, based on customer-specific backup policies.</li> <li>Rather than relying on deduplication methods of on-premise solutions, InftyDedup deduplicates data using the cloud infrastructure. (也就是说连 dedup 计算本身都可以做到不依赖本身的算力，读到现在我猜是也可以租 VM。) This is done periodically in batches before actually transferring data to the cloud, which, among others, enables the dynamic allocation of cloud resources.</li> </ul> <h2 id="background">Background</h2> <h3 id="deduplication-storage">Deduplication Storage</h3> <p>Firstly, the data stream is chunked into small immutable blocks of size from 2 KB to 128 KB. Secondly, each block receives a fingerprint, for instance, by computing the SHA-256 hash of the block’s data. Finally, the fingerprint is compared with other fingerprints in the system, and if the fingerprint is unique, the block’s data is written.</p> <p>The deduplicated blocks are typically organized in a directed acyclic graph. Each file has its root block corresponding to a vertex that references other blocks.</p> <h3 id="lifecycle-of-backups">Lifecycle of Backups</h3> <p>两个约束：</p> <ul> <li>the data should be available quickly in case of a disaster <ul> <li>e.g. Recovery Point Objectives of seconds and Recovery Time Objectives of minutes</li> </ul> </li> <li>older versions of backups need to be stored for weeks, months, or even years <ul> <li>backups are often moved to cheaper storage after a specific time</li> </ul> </li> </ul> <h3 id="cloud-storage">Cloud Storage</h3> <figure> <picture> <img src="/assets/img/fig/InftyDedup-fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="inftydedup-architecture">InftyDedup Architecture</h2> <p>TBD</p> <h2 id="评论">评论</h2> <ul> <li>我们能认为用户级别的 dedup 是在榨取 cloud providers 的利润吗？因为他们肯定也会做 dedup。</li> </ul>]]></content><author><name></name></author><category term="paperreading"/><category term="Cloud"/><category term="Deduplication"/><summary type="html"><![CDATA[InftyDedup: Scalable and Cost-Effective Cloud Tiering with Deduplication]]></summary></entry><entry><title type="html">GL-Cache (FAST 23) 论文阅读</title><link href="https://miraiwu.github.io/paperreading/2023/02/26/GL-Cache-(FAST-23).html" rel="alternate" type="text/html" title="GL-Cache (FAST 23) 论文阅读"/><published>2023-02-26T00:00:00+08:00</published><updated>2023-02-26T00:00:00+08:00</updated><id>https://miraiwu.github.io/paperreading/2023/02/26/GL-Cache%20(FAST%2023)</id><content type="html" xml:base="https://miraiwu.github.io/paperreading/2023/02/26/GL-Cache-(FAST-23).html"><![CDATA[<p><a href="https://www.usenix.org/system/files/fast23-yang.pdf">GL-Cache: Group-level learning for efficient and high-performance caching</a></p> <h2 id="introduction">Introduction</h2> <p>“learned caches”: employed machine learning to improve cache evictions</p> <ul> <li>object-level learning (e.g. LRB): learns the next access time for each object using dozens of object features and evicts the object with the furthest predicted request time <ul> <li>significant computation and storage overheads</li> <li>Zipf distributions “most objects only get a limited number of requests. This leads to limited object-level information for learning”</li> </ul> </li> <li>learning-from-distribution (e.g. LHD): models request probability distributions to inform eviction decisions <ul> <li>必须 random sample，利用的信息有限，绝大部分 object 在 distribution 的尾部、信息很少</li> </ul> </li> <li>learning-from-simple-experts (e.g. LeCaR and Cacheus): performs evictions by choosing eviction candidates recommended by experts (e.g., LRU and LFU), and updates experts’ weights based on their past performance on the workload <ul> <li>A delay exists between a bad eviction and an update on the expert’s weight (“delayed rewards” in reinforcement learning).</li> </ul> </li> </ul> <p>Group-level Learned Cache:</p> <ul> <li>cluster similar objects into groups using write time</li> <li>evict the least useful groups using a merge-based eviction</li> <li>introduce a group utility function to rank groups</li> </ul> <h2 id="background-and-motivation">Background and motivation</h2> <p>Questions:</p> <ul> <li>“Object-level learning needs to sample objects and perform inference at each write (eviction). For example, LRB samples 32 objects and copies their features to a matrix for inference for each eviction. In our measurement, each evic- tion (including feature copy, inference, and ranking) takes <strong>200 μs</strong> on one CPU core, indicating that the cache can evict at most 5,000 objects on a single core per second.” 200 us 这实验是怎么做的，这数字怎么这么大</li> </ul> <h2 id="gl-cache-group-level-learned-cache">GL-Cache: Group-level learned cache</h2> <ul> <li>Grouping by write time <ul> <li>PS: allow an efficient implementation using a log-structured cache</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/fig/GLCache-fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>Learning object-group utility <ul> <li>gradient boosting machines (GBM) because tree models do not require feature normalization</li> <li>formulate the learning task as a regression problem that minimizes the mean square loss (L2) of object-group utilities</li> <li>A sampled group may be evicted before being used for training. Such evictions halt the tracking of group utility. GL-Cache keeps ghost entries for objects which have not been factored into group utility.</li> <li>一次扔掉最差的 group 附近的 \(N_{group}\) 个 group，这个参数可能设置成整体的 1% 之类的</li> </ul> </li> </ul> <p>Questions:</p> <ul> <li>如果有好几个 application 同时写入，凭什么说有相似的 write-time 的 object 相似呢？</li> <li>“GL-Cache generates new training data by sampling cached object groups, and it copies the features of the sampled groups into a pre-allocated memory region” 这不是也有 random sampling (v.s. learning-from-distribution)?</li> </ul> <h2 id="evaluation">Evaluation</h2> <p>放两个有意思的图：</p> <figure> <picture> <img src="/assets/img/fig/GLCache-fig2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/fig/GLCache-fig3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="paperreading"/><category term="Cache"/><category term="ML"/><summary type="html"><![CDATA[GL-Cache: Group-level learning for efficient and high-performance caching]]></summary></entry><entry><title type="html">Chandy-Lamport (TOCS 85) 论文阅读</title><link href="https://miraiwu.github.io/paperreading/2023/01/27/Chandy-Lamport-(TOCS-85).html" rel="alternate" type="text/html" title="Chandy-Lamport (TOCS 85) 论文阅读"/><published>2023-01-27T00:00:00+08:00</published><updated>2023-01-27T00:00:00+08:00</updated><id>https://miraiwu.github.io/paperreading/2023/01/27/Chandy-Lamport%20(TOCS%2085)</id><content type="html" xml:base="https://miraiwu.github.io/paperreading/2023/01/27/Chandy-Lamport-(TOCS-85).html"><![CDATA[<p><a href="https://lamport.azurewebsites.net/pubs/chandy.pdf">Distributed Snapshots: Determining Global States of Distributed Systems</a></p> <h2 id="评论">评论</h2> <p><a href="https://zhuanlan.zhihu.com/p/53482103">这篇文章</a> 比较简洁地过了一遍。</p>]]></content><author><name></name></author><category term="paperreading"/><category term="distributed system"/><summary type="html"><![CDATA[Distributed Snapshots: Determining Global States of Distributed Systems]]></summary></entry><entry><title type="html">Time, Clocks, and Ordering (78) 论文阅读</title><link href="https://miraiwu.github.io/paperreading/2023/01/24/Time,-Clocks,-and-Ordering-(78).html" rel="alternate" type="text/html" title="Time, Clocks, and Ordering (78) 论文阅读"/><published>2023-01-24T00:00:00+08:00</published><updated>2023-01-24T00:00:00+08:00</updated><id>https://miraiwu.github.io/paperreading/2023/01/24/Time,%20Clocks,%20and%20Ordering%20(78)</id><content type="html" xml:base="https://miraiwu.github.io/paperreading/2023/01/24/Time,-Clocks,-and-Ordering-(78).html"><![CDATA[<p><a href="https://dl.acm.org/doi/10.1145/359545.359563">Time, clocks, and the ordering of events in a distributed system</a></p> <h2 id="评论">评论</h2> <p>不愧是 Leslie Lamport，直接放<a href="http://zhangtielei.com/posts/blog-time-clock-ordering.html">这篇解析</a>，总结得真好啊</p>]]></content><author><name></name></author><category term="paperreading"/><category term="distributed system"/><summary type="html"><![CDATA[Time, clocks, and the ordering of events in a distributed system]]></summary></entry><entry><title type="html">XRP (OSDI 22) 论文阅读</title><link href="https://miraiwu.github.io/paperreading/2023/01/22/XRP-(OSDI-22).html" rel="alternate" type="text/html" title="XRP (OSDI 22) 论文阅读"/><published>2023-01-22T00:00:00+08:00</published><updated>2023-01-22T00:00:00+08:00</updated><id>https://miraiwu.github.io/paperreading/2023/01/22/XRP%20(OSDI%2022)</id><content type="html" xml:base="https://miraiwu.github.io/paperreading/2023/01/22/XRP-(OSDI-22).html"><![CDATA[<p><a href="https://www.usenix.org/conference/osdi22/presentation/zhong">XRP: In-Kernel Storage Functions with eBPF</a></p> <h2 id="abstract">Abstract</h2> <p>We present XRP, a framework that allows applications to execute user-defined storage functions, such as index lookups or aggregations, from an eBPF hook in the NVMe driver, safely bypassing most of the kernel’s storage stack.</p> <h2 id="introduction">Introduction</h2> <p>Complete kernel bypass (SPDK): force applications to imple- ment their own file systems, to forgo isolation and safety, and to poll for I/O completion which wastes CPU cycles when I/O utilization is low; suffer from high average and tail latencies and severely reduced throughput when the schedulable thread count exceeds the number of available cores.</p> <p>BPF is an OS-supported mechanism that ensures isolation, does not lead to low utilization due to busy-waiting, and allows a large number of threads or processes to share the same core, leading to better overall utilization.</p> <p>In order to maximize its performance benefit, XRP uses a hook in the NVMe driver’s interrupt handler, thereby bypassing the kernel’s block, file system and system call layers. This allows XRP to trigger BPF functions directly from the NVMe driver as each I/O completes, enabling quick resubmission of I/Os that traverse other blocks on the storage device.</p> <h2 id="background-and-motivation">Background and Motivation</h2> <p>Software is now the storage bottleneck:</p> <figure> <picture> <img src="/assets/img/fig/XRP-fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="design-callenges-and-principles">Design Callenges and Principles</h2> <ul> <li>Challenge 1: address translation and security. <ul> <li>the BPF function could access any block on the device, including blocks that belong to a file that the user does not have permissions to access.</li> </ul> </li> <li>Challenge 2: concurrency and caching. <ul> <li>A write issued from the file system will only be reflected in the page cache, which is not visible to XRP. In addition, any writes that modify the layout of the data structure (e.g., modify the pointers to the next block) that are issued concurrently to read requests could lead XRP to accidentally fetch the wrong data. Both of these could be addressed by locking, but accessing locks from within the NVMe interrupt handler may be expensive.</li> </ul> </li> <li>Observation: most on-disk data structures are stable, e.g. LSM trees.</li> <li>Design principles. <ul> <li>One file at a time: only issue chained resubmission on a single file.</li> <li>Stable data structures: whose layout (i.e. pointers) remain immutable for a long period of time (i.e. seconds or more); do not plan to support operations that require locks during the traversal or iteration of data structures.</li> <li>User-managed caches: 不能有 os page cache</li> <li>Slow path fallback (best-effort)</li> </ul> </li> </ul> <h2 id="xrp-design-and-implementation">XRP Design and Implementation</h2> <p>暂略</p> <h2 id="case-studies">Case Studies</h2> <p>BPF-KV 这个比较有意思，如下：</p> <ul> <li>BPF-KV is designed to store a large number of small objects and to provide good read performance even under uniform access patterns. BPF-KV uses a B+-tree index to find the location of objects, and the objects themselves are stored in an unsorted log. For simplicity, BPF-KV uses fixed-sized keys (8 B) and values (64 B).</li> <li>Consider the case where BPF-KV is used to store 10 billion 64 B objects. In BPF-KV’s index, each node is 512 B (matching the access granularity of the Optane SSD); hence, the tree has a fanout of 31 (i.e. each internal node can store pointers to 31 children). Therefore, 10 billion objects would require an index with 8 levels. Fitting 6 index levels in DRAM is expensive and would require 14 GB, while fitting 7 levels or more becomes prohibitively expensive (437 GB of DRAM or more). So, to support a large number of keys, BPF-KV would require at the minimum 3-4 I/Os from storage for each lookup, including a final I/O to fetch the actual key-value pair from disk.</li> <li>Also note that having a hard memory budget for caching the index is common in many real-world key-value stores, since the index cache often competes with other parts of the system that need memory, such as filters and the object cache.</li> </ul> <figure> <picture> <img src="/assets/img/fig/XRP-fig2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="评价">评价</h2> <p>use case 到底够不够广泛我不确定；software overhead 能看见的、算足够大的场景可能也就 Optane P5800X 了；assumption 有点多，比如 target static data structures，感觉有点困难。</p>]]></content><author><name></name></author><category term="paperreading"/><category term="storage"/><summary type="html"><![CDATA[XRP: In-Kernel Storage Functions with eBPF]]></summary></entry><entry><title type="html">RPC (TOCS 84) 论文阅读</title><link href="https://miraiwu.github.io/paperreading/2023/01/21/RPC-(TOCS-84).html" rel="alternate" type="text/html" title="RPC (TOCS 84) 论文阅读"/><published>2023-01-21T00:00:00+08:00</published><updated>2023-01-21T00:00:00+08:00</updated><id>https://miraiwu.github.io/paperreading/2023/01/21/RPC%20(TOCS%2084)</id><content type="html" xml:base="https://miraiwu.github.io/paperreading/2023/01/21/RPC-(TOCS-84).html"><![CDATA[<p><a href="https://dl.acm.org/doi/10.1145/2080.357392">Implementing Remote Procedure Calls</a></p> <h2 id="introduction">Introduction</h2> <p>Major issues facing the designer of an RPC facility include: the <strong>precise semantics</strong> of a call in the presence of machine and communication failures; the semantics of address-containing arguments in the (possible) absence of a shared address space; integration of remote calls into existing (or future) programming systems; <strong>binding (how a caller determines the location and identity of the callee)</strong>; suitable protocols for transfer of data and control between caller and callee; and how to provide data integrity and security (if desired) in an open communication network.</p> <p>Our hope is that by providing communication with almost as much ease as local procedure calls, people will be encouraged to build and experiment with distributed applications. RPC will, we hope, remove unnecessary difficulties, leaving only the fundamental difficulties of building distributed systems: timing, independent failure of components, and the coexistence of independent execution environments.</p> <p>A principle that we used several times in making design choices isthat the semantics of remote procedure calls should be as <strong>close</strong> as possible <strong>to those of local (single-machine) procedure calls</strong>. For example, we chose to have no time-out mechanism limiting the duration of a remote call(in the absence of machine or communication failures), whereas most communication packages consider this a worthwhile feature. Our argument is that local procedure calls have no time-out mechanism, and our languages include mechanisms to abort an activity as part of the parallel processing mechanism. Designing a new time-out arrangement just for RPC would needlessly complicate the programmer’s world.</p> <figure> <picture> <img src="/assets/img/fig/RPC-fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="binding">Binding</h2> <p>There are two aspects to binding which we consider in turn. First, how does a client of the binding mechanism specify what he wants to be bound to? Second, how does a caller determine the machine address of the callee and specify to the callee the procedure to be invoked? The first is primarily a question of naming and the second a question of location.</p> <p>There are two parts to the name of an interface: the type and the instance. For example, the type of an interface might correspond to the abstraction of “mail server,” and the instance would correspond to some particular mail server selected from many.</p> <figure> <picture> <img src="/assets/img/fig/RPC-fig2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="packet-level-transport-protocol">PACKET-LEVEL TRANSPORT PROTOCOL</h2> <p>后略</p>]]></content><author><name></name></author><category term="paperreading"/><category term="distributed system"/><summary type="html"><![CDATA[Implementing Remote Procedure Calls]]></summary></entry><entry><title type="html">Understand-Timeout (IC2E 18) 论文阅读</title><link href="https://miraiwu.github.io/paperreading/2023/01/15/Understand-Timeout-(IC2E-18).html" rel="alternate" type="text/html" title="Understand-Timeout (IC2E 18) 论文阅读"/><published>2023-01-15T00:00:00+08:00</published><updated>2023-01-15T00:00:00+08:00</updated><id>https://miraiwu.github.io/paperreading/2023/01/15/Understand-Timeout%20(IC2E%2018)</id><content type="html" xml:base="https://miraiwu.github.io/paperreading/2023/01/15/Understand-Timeout-(IC2E-18).html"><![CDATA[<p><a href="https://tingdai.github.io/files/understanding_IC2E18.pdf">Understanding Real-World Timeout Problems in Cloud Server Systems</a></p> <p>我是 find-bug 小白，所以会记录很多基础信息：）</p> <h2 id="abstract">Abstract</h2> <p>Timeouts are commonly used to handle unexpected failures in distributed systems.</p> <ul> <li>study 11 commonly used cloud server systems (e.g., Hadoop, HDSF, Spark, Cassandra, etc.)</li> <li>root causes of timeout problems include misused timeout, missing timeout, improper timeout handling, unnecessary timeout, and clock drifting</li> </ul> <h2 id="introduction">Introduction</h2> <p>Timeout mechanism can be used in both intra-node and inter-node communication failover. For example, when a component C1 sends a request to another component C2, C1 sets a timer timeout value and waits for the response from C2 until the timer expires. In case C2 fails or a message loss occurs, C1 can break out of the waiting state triggered by the timeout event and take proper actions (e.g., retrying or skipping) accordingly.</p> <p>一个例子：In 2015, Amazon DynamoDB service was down for five hours. The service outage is caused by a timeout bug in the metadata server. When the metadata server was already overloaded, the new requests from storage servers to the metadata server failed due to timeout. Storage servers kept retrying, causing further failures and retries, creating a cascading failure.</p> <figure> <picture> <img src="/assets/img/fig/Understand-Timeout-fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>Timeout bug root causes: Our study shows that real-world timeout problems are caused by a variety of reasons including <ul> <li>1) misused timeout value (47%) where a timeout variable is misconfigured, ignored or incorrectly reused;</li> <li>2) missing timeout checking (31%) where inter-component communication lacks timeout protection;</li> <li>3) improper timeout handling where a timeout event is handled by inappropriate retries or aborts;</li> <li>4) unnecessary timeout where a timeout is used for a function call which does not need timeout protection;</li> <li>5) clock drifting where timeout problems are caused by asynchronous clocks between distributed hosts.</li> </ul> </li> <li>Timeout bug impact</li> <li>Timeout bug diagnosis: Our study shows that real-world timeout bugs are difficult to diagnose: 60% timeout bugs do not produce any error message and 12% timeout bugs produce misleading error messages.</li> </ul> <h2 id="methodology">Methodology</h2> <h2 id="root-cause-analysis">Root cause analysis</h2> <p>很多例子！非常有意思！</p> <h2 id="评论">评论</h2> <p>well-written! 下一篇准备读 follow-up 的解决方案 :)</p>]]></content><author><name></name></author><category term="paperreading"/><category term="find-bug"/><category term="timeout"/><summary type="html"><![CDATA[Understanding Real-World Timeout Problems in Cloud Server Systems]]></summary></entry><entry><title type="html">Paxos (PODC 07) 论文阅读</title><link href="https://miraiwu.github.io/paperreading/2023/01/12/Paxos-(PODC-07).html" rel="alternate" type="text/html" title="Paxos (PODC 07) 论文阅读"/><published>2023-01-12T00:00:00+08:00</published><updated>2023-01-12T00:00:00+08:00</updated><id>https://miraiwu.github.io/paperreading/2023/01/12/Paxos%20(PODC%2007)</id><content type="html" xml:base="https://miraiwu.github.io/paperreading/2023/01/12/Paxos-(PODC-07).html"><![CDATA[<p><a href="https://dl.acm.org/doi/pdf/10.1145/1281100.1281103">Paxos Made Live - An Engineering Perspective</a></p> <h2 id="abstract">Abstract</h2> <ul> <li>describe our experience in building a fault-tolerant database using the Paxos <strong>consensus algorithm</strong> (non-trivial)</li> <li>describe selected algorithmic and engineering problems encountered, and the solutions we found for them</li> </ul> <h2 id="introduction">Introduction</h2> <h2 id="background">Background</h2> <p>Chubby is a fault-tolerant system at Google that provides a distributed locking mechanism and stores small files.</p> <p>Several Google systems – such as the Google Filesystem (GFS) and Bigtable – use Chubby for distributed coordination and to store a small amount of metadata. A typical Chubby cell consists of five replicas, running the same code, each running on a dedicated machine. Every Chubby object (e.g., a Chubby lock, or file) is stored as an entry in a database. It is this database that is replicated. At any one time, one of these replicas is considered to be the “master”.</p> <p>Chubby clients (such as GFS and Bigtable) contact a Chubby cell for service. The master replica serves all Chubby requests. If a Chubby client contacts a replica that is not the master, the replica replies with the master’s network address. The Chubby client may then contact the master. If the master fails, a new master is automatically elected, which will then continue to serve traffic based on the contents of its local copy of the replicated database. Thus, the replicated database ensures continuity of Chubby state across master failover.</p> <p>Chubby 很重要，相比原先直接拿一个第三方实现 (“3DB”)，作者们决定自己做一个很确定正确性的系统。</p> <h2 id="architecture-outline">Architecture outline</h2> <figure> <picture> <img src="/assets/img/fig/Paxos-fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We devoted effort to designing clean interfaces separating the Paxos framework, the database, and Chubby. We did this partly for clarity while developing this system, but also with the intention of reusing the replicated log layer in other applications.</p> <h2 id="on-paxos">On Paxos</h2> <figure> <picture> <img src="/assets/img/fig/Paxos-fig2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>略</p> <h2 id="评论">评论</h2> <p>剩下的看<a href="https://fuzhe1989.github.io/2021/03/25/paxos-made-live/">这篇</a>吧</p>]]></content><author><name></name></author><category term="paperreading"/><category term="fault-tolerance"/><summary type="html"><![CDATA[Paxos Made Live - An Engineering Perspective]]></summary></entry><entry><title type="html">Dynacache (HotCloud 15) 论文阅读</title><link href="https://miraiwu.github.io/paperreading/2023/01/10/Dynacache-(HotCloud-15).html" rel="alternate" type="text/html" title="Dynacache (HotCloud 15) 论文阅读"/><published>2023-01-10T00:00:00+08:00</published><updated>2023-01-10T00:00:00+08:00</updated><id>https://miraiwu.github.io/paperreading/2023/01/10/Dynacache%20(HotCloud%2015)</id><content type="html" xml:base="https://miraiwu.github.io/paperreading/2023/01/10/Dynacache-(HotCloud-15).html"><![CDATA[<p><a href="https://www.usenix.org/conference/hotcloud15/workshop-program/presentation/cidon">Dynacache: Dynamic Cloud Caching</a></p> <h2 id="abstract">Abstract</h2> <ul> <li>Dynacache, a cache controller, significantly improves the hit rate of web applications, by profiling applications and dynamically tailoring memory resources and eviction policies.</li> <li>Dynacache allows Memcached operators to better plan their resource allocation and manage server costs, by estimating the cost of cache hits as a function of memory.</li> </ul> <p>总的来说就是 profiling+tuning 很有提升空间、是值得的，Dynacache 通过更好地平衡 slab 之间的分配来优化性能。</p> <h2 id="introduction">Introduction</h2> <p>很多人都不觉得 simplicity 和 performance 也是个重要的 trade-off，这里提到了我还挺惊喜的：”While a dynamic application-aware cache is conceptually appealing, is it worth the additional complexity in practice?”</p> <figure> <picture> <img src="/assets/img/fig/Dynacache-fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="memcachier-trace-analysis">MEMCACHIER TRACE ANALYSIS</h2> <h2 id="design">Design</h2> <h3 id="practical-profiling">Practical Profiling</h3> <p>没看懂：</p> <p>“Dynacache uses a bucketing scheme similar to Mimir. In this bucketing scheme, instead of keeping track of a shadow eviction queue, there is a linked list of buckets, each containing a fixed number of items. Each incoming request enters the top bucket, and when the top bucket is filled, we remove the bucket at the end of the queue. We maintain a hash function that maps each item to the bucket in which it is stored. We can estimate the stack distance of an in- coming request, by summing the size of all buckets that appear in the queue before it, and adding it to the size of its own bucket divided by 2. This stack distance computation algorithm is much faster than the naïve method, since its complexity is O(B), where B is the number of buckets. For Memcachier, we utilized 100 buckets with 100 keys each.”</p> <h2 id="评论">评论</h2> <ul> <li>感觉跟 LAMA 撞了，而且总体上被 LAMA 高级替代了</li> </ul>]]></content><author><name></name></author><category term="paperreading"/><category term="Cache"/><summary type="html"><![CDATA[Dynacache: Dynamic Cloud Caching]]></summary></entry><entry><title type="html">LAMA (ATC 15) 论文阅读</title><link href="https://miraiwu.github.io/paperreading/2023/01/09/LAMA-(ATC-15).html" rel="alternate" type="text/html" title="LAMA (ATC 15) 论文阅读"/><published>2023-01-09T00:00:00+08:00</published><updated>2023-01-09T00:00:00+08:00</updated><id>https://miraiwu.github.io/paperreading/2023/01/09/LAMA%20(ATC%2015)</id><content type="html" xml:base="https://miraiwu.github.io/paperreading/2023/01/09/LAMA-(ATC-15).html"><![CDATA[<p><a href="https://www.usenix.org/conference/atc15/technical-session/presentation/hu">LAMA: Optimized Locality-aware Memory Allocation for Key-value Cache</a></p> <h2 id="abstract">Abstract</h2> <p>以前有 slab calcification 问题。</p> <p>This paper introduces locality-aware memory allocation (LAMA), which solves the problem by first analyzing the locality of the Memcached requests and then repartitioning the memory to minimize the miss ratio and the average response time.</p> <h2 id="introduction">Introduction</h2> <p>slab allocation 背景知识：</p> <p>Memcached splits the memory cache space into different classes to store variable-sized objects as items. Initially, each class obtains its own memory space by requesting free slabs, 1MB each, from the allocator. Each allocated slab is divided into slots of equal size. According to the slot size, the slabs are categorized into different classes, from Class 1 to Class n, where the slot size increases exponentially.</p> <p>目前的问题是 slab calcification 未能得到很好的解决：In the presence of workload changing, default Memcached server may suffer from a problem called slab calcification, in which the allocation cannot be adjusted to fit the change of access pattern as the old slab allocation may not work well for the new workload.</p> <p>解决方法是为每个 class 产生 MRC，之后平衡 space allocation：</p> <p>This study provides a low-overhead yet accurate method to model data locality and generate miss ratio curves (MRCs). With MRCs for all classes, the overall Memcached performance can be modelled in terms of different class space allocations, and it can be optimized by adjusting individual classes’ allocation.</p> <h2 id="background">Background</h2> <h3 id="memory-allocation-in-memcached">Memory Allocation in Memcached</h3> <p>五个现有的算法都有点差的惊人：</p> <ul> <li>Memcached Default Design: based on the number of items arriving in different classes during the cold start period <ul> <li>不能对抗任何 workload 变化</li> </ul> </li> <li>Automove: In every 10 seconds window, if a class takes the highest number of evictions in <strong>three</strong> consecutive monitoring windows, a new slab is reassigned to it; the new slab is taken from the class that has no evictions in the last three monitoring stages. <ul> <li>需要 30 秒连续没有访问的 class 存在才能做 eviction</li> </ul> </li> <li>Twitter Policy: random eviction, aggressive</li> <li>Periodic Slab Allocation (PSA): the number of requests of Class i is denoted as Ri and the number of slabs allocated to it is denoted as Si. The risk of moving one slab away from Class i is denoted as <strong>Ri/Si</strong>. Every M misses, PSA moves one slab from the class with the lowest risk to the class with the largest number of misses. <ul> <li>问题是这两个 metrics 不一致，low risk（访问数的平均数）和 high miss（不命中的总数）可能是同一些 classes</li> </ul> </li> <li>Facebook Policy [age balancing]: if a class is currently evicting items, and the next item to be evicted was used at least 20% more recently than the average least recently used item of all other classes, this class is identified as needing more memory (其实是 approximate LRU)</li> </ul> <h3 id="the-footprint-theory">The Footprint Theory</h3> <p>略</p> <h2 id="locality-aware-memory-allocation">Locality-aware Memory Allocation</h2> <p>Goal 是 miss ratio (MR) 或者 average request time (ART)</p> <p>用 dynamic programming 解 “The time complexity of the optimization is O(n * MAX2), where n is the number of size classes and MAX is the total number of slabs.”</p> <p>这里有嵌套<a href="https://ieeexplore.ieee.org/document/6113843">另一篇文章</a>的 MRC 公式，我得补一下原论文。</p> <h2 id="evaluation">Evaluation</h2> <p>这部分很有意思：”The MRC analysis is performed by a separate thread. Each analysis samples recent 20 million requests which are stored using a circular buffer. The buffer is shared by all Memcached threads and protected by a mutex lock for atomic access. During the analysis, it uses a hash table to record the last access time. The cost depends on the size of the items being analyzed. It is 3% - 4% of all memory depending for the workload we use.” 对于高吞吐压力的情况，这个 circular buffer 是 sequential 结构，可能带来不小的延迟/等待。</p> <p>结果首先注意到 miss rate 和 response time 趋势很近，符合预期；其次 Twemcache 的 random policy 表现不佳，default 和几乎不怎么移动的 automove 表现也不佳；Facebook 的 age balancing 应该说比较间接，表现排在中间的样子；PSA 是和 LAMA 表现得最接近的 baseline，也说明 low risk 和 high miss count 作为 metrics 挺好的：</p> <figure> <picture> <img src="/assets/img/fig/LAMA-fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>另一个值得注意的是 LAMA 有两个略显丑陋的参数，the repartitioning interval M, which is the number of items accesses before repartitioning; and the reassignment upper bound N, which is the maximal number of slabs reassigned at repartitioning。</p> <p>LAMA 还声称 MRC overhead 很小，相比 reuse-distance-based MRC generation（但没说具体用了什么 RD 算法），这个算法复杂度我下一篇文章对比一下：</p> <figure> <picture> <img src="/assets/img/fig/LAMA-fig2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="评论">评论</h2> <p>我是当作一篇工程文章看的。虽然 target 在 Memcached 已经看到的很大的收益，但是如果能 claim larger impact，比如说，有没有可能 MRC + DP 可以做得更有应用场景呢？反正我如果要用 MRC + DP 解什么问题，我会考虑这篇文章的处理思路以及观察到的效果。</p>]]></content><author><name></name></author><category term="paperreading"/><category term="Cache"/><category term="MRC-Generation"/><summary type="html"><![CDATA[LAMA: Optimized Locality-aware Memory Allocation for Key-value Cache]]></summary></entry></feed>